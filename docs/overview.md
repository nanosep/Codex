# Project Overview (Plain-English Guide)

## What this app is

This project is a learning tool for testing a simple machine-learning workflow on daily market-style data. It is not a live trading system. Think of it as a simulator: you feed it a table of historical days, it applies fixed rules, and it reports what would have happened under those rules.

A **dataset** in this app means a table of daily rows, each row describing that day’s price range and trading activity. The table is generic: it can represent a stock, an ETF, a crypto pair, or another asset, but the file itself does not claim a specific instrument by name.

Each row uses **OHLCV** (Open, High, Low, Close, Volume):

- **Open**: the first price of that day.
- **High**: the highest price reached that day.
- **Low**: the lowest price reached that day.
- **Close**: the last price of that day.
- **Volume**: how much was traded that day.

The app runs offline from CSV files (plain text table files). That makes results reproducible and avoids dependence on live network data feeds.

## Dataset modes in the app

The app has three data source modes. These names appear exactly in the sidebar:

1. **Demo synthetic (balanced-ish)** (`data/sample_daily_demo.csv`)
2. **Realistic synthetic (market-like)** (`data/sample_daily_realistic.csv`)
3. **Upload CSV**

### 1) Demo synthetic (balanced-ish)

“Synthetic” means the data is artificially generated by code instead of downloaded from an exchange. This demo dataset is designed to make the full pipeline reliable in demos: both positive and negative outcomes are present often enough that training and backtesting can run without manual tweaking.

Use this mode when you want a quick “does the pipeline work end-to-end?” check.

### 2) Realistic synthetic (market-like)

This is also synthetic, but shaped to behave more like real markets: more uneven moves, more variability over time, and occasional larger shocks. Outcomes can be harder and less balanced than the demo set.

Use this mode when you want a tougher, more realistic stress test while staying fully offline and reproducible.

### 3) Upload CSV

Use your own daily data file. Required columns are:

- `date`
- `open`
- `high`
- `low`
- `close`
- `volume`

Right now, the app expects daily rows only.

## User flow: Step 1 to Step 6

### Step 1 — Rule-based backtest

A **backtest** means replaying historical days as if you traded them at that time, using rules fixed in advance. In Step 1, the app uses hardcoded indicator rules to decide when to enter and exit.

Outputs:

- trade list (`trades.csv`)
- equity curve (`equity.csv`)
- chart with entries/exits (`chart.png`)

### Step 2 — Labeling (“what counts as a win”)

This step defines the target for machine learning. It checks future days after each row:

- **TP (Take Profit)**: a profit target.
- **SL (Stop Loss)**: a loss limit.
- **Horizon**: how many future days are checked.

A row is labeled as a “clean win” only if TP is reached within the horizon and SL is never touched first. This creates a clear yes/no training target.

Output:

- labeled rows (`labeled.csv`)

### Step 3 — Features + baseline model

A **feature** is a numeric input built from past data (for example short-term return or trend distance). Features are trailing-only, meaning they are calculated from current and earlier rows, not future rows.

The baseline model is **Logistic Regression**, a classic probability model that outputs a chance between 0 and 1 for the positive label.

Outputs:

- feature table (`features.csv`)
- trained model (`model.joblib`)
- predictions (`predictions.csv`)
- metrics (`metrics.json`)

### Step 4 - Model Zoo (compare models)

A **model zoo** means testing several model types under the same data and labels. In plain terms, all models look at the same feature table but use different math to convert those inputs into a probability. This project compares four models and picks one winner using time-series-safe validation.

"Time-series-safe" means no random shuffle of dates. Earlier data is used to predict later data, which is closer to real deployment behavior.

#### Model Zoo (Step 4), in plain terms

- **Logistic Regression**: a weighted scorecard model. It is simple, stable, and a good baseline.
- **K-Nearest Neighbors (KNN)**: compares each row to similar past rows and learns from those neighbors.
- **Random Forest**: many decision trees vote together, which helps capture more complex patterns.
- **Gaussian Naive Bayes**: a fast probability model with strong simplifying assumptions.

All four models see the same inputs and target. The difference is how each model turns those inputs into a prediction.

#### How each model computes probability

- **Logistic Regression**: computes a weighted sum of features, then applies an S-shaped function to get a value from 0 to 1.
- **KNN**: finds the nearest past rows in feature space, then uses the fraction of positive neighbors as probability.
- **Random Forest**: builds many trees and averages their class votes into a probability.
- **Gaussian Naive Bayes**: estimates feature distributions per class and combines those likelihoods into class probability.

Outputs:

- candidate comparison (`model_candidates.json`)
- selected best model (`best_model.joblib`)
- best-model metrics and predictions
- model card (`best_model_card.md`)

### Step 5 — ML signals + TP/SL/horizon backtest

The trained model gives a **probability** per day. A **threshold** is the cutoff used to turn probability into a trade signal (for example, signal = 1 if probability >= 0.50).

When signal is on and there is no open trade, the system enters one long trade. It then exits by fixed TP/SL/horizon rules. The MVP allows one open position at a time.

Outputs:

- ML signals (`ml_signals.csv`)
- ML trades (`ml_trades.csv`)
- ML equity (`ml_equity.csv`)
- ML chart (`ml_chart.png`)
- ML summary (`ml_summary.json`)

### Step 6 — Threshold Policy (work in progress)

Using 0.50 by default is convenient, but arbitrary. Step 6 searches many thresholds and chooses one on training data only, then evaluates once on test data to reduce leakage risk.

The goal is to freeze a threshold policy that is more robust than guessing 0.50 by default.

Outputs:

- threshold search table (`threshold_search.csv`)
- chosen threshold (`best_threshold.json`)
- test-only backtest artifacts (`test_*`)

## Assumptions in this system

These are explicit design assumptions in the current implementation:

- Daily timeframe only.
- Entry is assumed at the day’s close price.
- TP/SL checks use each day’s high/low values.
- One open trade at a time (no overlapping positions).
- Intraday ambiguity rule: if TP and SL are both hit on the same day, treat it as SL first (conservative).
- No fees and no slippage in this MVP.
- Offline-first operation from CSV files; no live broker connections.

## What this does NOT do

- It is **not financial advice**.
- It is **not live trading**.
- It does **not include execution costs** (fees/slippage).
- It is **not proof of profitability**.
- It does not guarantee future performance on real assets.

## How to read results without over-trusting them

Use the output as a quality check, not a final truth:

- If positive labels are extremely rare, model metrics can look unstable.
- If there are very few trades, backtest conclusions are weak.
- If drawdown is high, risk may be too high even when average return looks good.
- If results change sharply between demo and realistic modes, your setup may be too sensitive.

A good habit is to compare:

1. Class balance in Step 2,
2. Model quality in Steps 3–4,
3. Strategy behavior in Steps 5–6.

If only one of these looks good and the others look weak, treat conclusions as provisional.

## Glossary

- **Dataset**: A table of daily rows with price and volume columns.
- **Time series**: Data ordered by time (oldest to newest).
- **OHLCV**: Open, High, Low, Close, Volume for each day.
- **Backtest**: Replay historical data using fixed trade rules.
- **TP (Take Profit)**: Profit target level for exiting a trade.
- **SL (Stop Loss)**: Loss limit level for exiting a trade.
- **Horizon**: Maximum number of future days to check after entry.
- **Label**: The target outcome used for training (here, clean win = 1).
- **Feature**: Numeric input built from historical data for the model.
- **Model**: Algorithm that maps features to a predicted probability.
- **Probability**: Model confidence between 0 and 1 for the positive class.
- **Threshold**: Probability cutoff used to trigger a signal.
- **Equity curve**: Running account value over time in the simulation.
- **Drawdown**: How far equity falls from a previous peak.
